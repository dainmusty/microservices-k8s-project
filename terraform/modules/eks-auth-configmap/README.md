# you can use the codes below to create your passwords in parameter store
eg. 1 db_user
# modules/ssm/main.tf
resource "aws_ssm_parameter" "db_user" {
  name  = var.db_user_parameter_name
  type  = var.db_user_parameter_type
  value = var.db_user_parameter_value

}

eg. 2
# SSM Parameter for Grafana Admin Password
resource "aws_ssm_parameter" "grafana_admin_password" {
  name        = "/grafana/admin/password"
  type        = "SecureString"
  value       = var.grafana_admin_password
  description = "Grafana admin password for cluster monitoring dashboard"
}
This exposes your actual password. create your password in parameter store in the console and put the value there before terraform apply

# Once the created, you can use data to retrieve your passwords
# Data sources for EKS cluster info

data "aws_eks_cluster" "eks" {
  name = var.cluster_name
}

data "aws_ssm_parameter" "db_access_parameter_name" {
  name = var.db_access_parameter_name
}

data "aws_ssm_parameter" "db_secret_parameter_name" {
  name = var.db_secret_parameter_name
}

data "aws_ssm_parameter" "key_parameter_name" {
  name = var.key_parameter_name
}


data "aws_ssm_parameter" "key_name_parameter_name" {
  name = var.key_name_parameter_name
}

# Note: You can inject this value into the Grafana Helm chart like below:
# data "aws_ssm_parameter" "grafana_admin_password" {
#   name            = "/grafana/admin/password"
#   with_decryption = true
# }
# 
# and then set it via:
# set {
#   name  = "adminPassword"
#   value = data.aws_ssm_parameter.grafana_admin_password.value
# }


 # Use secret mgr for your grafana password, it has the ff advantages
 | Feature               | SSM Parameter Store   | Secrets Manager      |
| --------------------- | --------------------- | -------------------- |
| Cost                  | âœ… Cheaper             | âŒ Higher             |
| Encryption            | âœ… With KMS            | âœ… Built-in           |
| Versioning            | âŒ No                  | âœ… Yes                |
| Rotation              | âŒ Manual              | âœ… Automatic          |
| Best for              | Config, passwords     | DB creds, tokens     |
| Grafana dynamic fetch | ğŸš§ Extra setup needed | âœ… Easier integration |

How mature teams do this (FYI)

In production setups:

aws-auth is bootstrapped once

Then removed from Terraform state

Managed manually or via RBAC migration

But your current pattern is perfectly valid and safe.

Final takeaway (important)

aws-auth is not configuration â€” it is ownership.
Ownership is established once, not continuously reconciled.

Youâ€™ve designed this the right way.

If you want, next I can:

Show how to fully decouple aws-auth from Terraform

Or design a clean CI â†’ Argo â†’ workload pipeline with zero cluster-admin rights

Just say the word ğŸ‘


This is a classic EKS authentication vs authorization issue, and your error message tells us exactly where the problem is ğŸ‘

Whatâ€™s happening (in plain terms)

aws eks update-kubeconfig âœ… worked â†’ kubeconfig is correct

kubectl âŒ fails with

the server has asked for the client to provide credentials

This means:

Your AWS IAM identity is authenticated to AWS, but it is NOT authorized in Kubernetes (RBAC).

EKS uses IAM for authentication
Kubernetes uses RBAC for authorization

The bridge between them is the aws-auth ConfigMap.

Root Cause (most likely)

The IAM role or user youâ€™re currently using (probably:

your local AWS profile or

the Terraform / GitHub Actions role)

âŒ is NOT mapped in aws-auth ConfigMap

So Kubernetes rejects all requests.

Quick confirmation

Run this (it will fail, but confirms the issue):

kubectl get nodes


If you see:

You must be logged in to the server


ğŸ‘‰ 100% aws-auth mapping issue.

Fix: Add your IAM role/user to aws-auth
Step 1: Identify who you are right now
aws sts get-caller-identity


Note either:

Arn: arn:aws:iam::<acct>:user/...

OR Arn: arn:aws:iam::<acct>:role/...

This is the identity Kubernetes must trust.

Step 2: Edit aws-auth ConfigMap

âš ï¸ You must do this using the cluster creator role (the role that created the EKS cluster).

kubectl edit configmap aws-auth -n kube-system

Step 3: Add your role (example)

If youâ€™re using an IAM role (recommended):

apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
    - rolearn: arn:aws:iam::651706774390:role/microservices-project-dev-tf-role
      username: admin
      groups:
        - system:masters


If youâ€™re using an IAM user:

data:
  mapUsers: |
    - userarn: arn:aws:iam::651706774390:user/Mustydain
      username: mustydain
      groups:
        - system:masters


ğŸ‘‰ system:masters = full cluster admin (fine for dev)

Step 4: Save & re-test
kubectl get nodes
kubectl -n argocd get secret argocd-initial-admin-secret \
  -o jsonpath="{.data.password}" | base64 -d


âœ… This should now work.

Important (GitHub Actions / Terraform tie-in)

Because youâ€™re using OIDC + Terraform + GitHub Actions:

You must also add the GitHub Actions IAM role to aws-auth, otherwise:

terraform apply succeeds

aws eks update-kubeconfig works

kubectl commands inside CI fail

Example:

- rolearn: arn:aws:iam::651706774390:role/github-actions-eks-role
  username: github-actions
  groups:
    - system:masters

Best Practice (recommended for your setup)

Since youâ€™re already advanced with:

Terraform

EKS

ArgoCD

GitHub Actions

ğŸ‘‰ Manage aws-auth with Terraform, not manually.

Typical module:

module "aws_auth" {
  source  = "terraform-aws-modules/eks/aws//modules/aws-auth"

  map_roles = [
    {
      rolearn = aws_iam_role.tf_role.arn
      username = "terraform"
      groups   = ["system:masters"]
    }
  ]
}

Summary
Symptom	Cause
kubeconfig updates but kubectl fails	IAM not mapped in aws-auth
â€œserver has asked for credentialsâ€	Kubernetes RBAC denies you
EKS console shows nodes Ready	Control plane is fine

If you want, next I can:

Generate the Terraform aws-auth module for your exact roles

Help you lock down access (no system:masters)

Fix this once and permanently for local + CI users

Short answer: yes â€” either add your IAM user OR assume the Terraform role locally.
Right now youâ€™re authenticated to AWS, but Kubernetes still doesnâ€™t trust who you are.

Letâ€™s break it down cleanly.

What your config tells me (important)

Your aws-auth Terraform config is actually correct ğŸ‘‡

map_roles = [
  # worker nodes
  {
    rolearn  = module.iam_core.node_group_role_arn
    username = "system:node:{{EC2PrivateDNSName}}"
    groups   = ["system:bootstrappers", "system:nodes"]
  },

  # Terraform / CI access
  {
    rolearn  = "arn:aws:iam::651706774390:role/microservices-project-dev-tf-role"
    username = "terraform"
    groups   = ["system:masters"]
  },
]


This means:

âœ… The Terraform role has full cluster-admin access
âŒ Your local IAM user is NOT mapped

Why kubectl still fails

When you run locally:

kubectl get nodes


Kubernetes checks:

â€œWho is this request coming from?â€

Answer: your local IAM identity, not the Terraform role.

Unless you explicitly assume this role:

arn:aws:iam::651706774390:role/microservices-project-dev-tf-role


Kubernetes sees an unmapped IAM user â†’ âŒ access denied.

Critical rule (EKS auth rule of thumb)

EKS access is identity-specific

Mapping a role does NOT grant access to a user unless the user assumes that role.

You have 2 correct options (pick ONE)
âœ… Option 1 (RECOMMENDED): Assume the Terraform role locally

This is the clean, production-grade approach.

Step 1: Add assume-role permissions to your IAM user

Your IAM user must be allowed to assume:

microservices-project-dev-tf-role


Policy example:

{
  "Effect": "Allow",
  "Action": "sts:AssumeRole",
  "Resource": "arn:aws:iam::651706774390:role/microservices-project-dev-tf-role"
}

Step 2: Configure AWS profile

~/.aws/config

[profile terraform-admin]
role_arn = arn:aws:iam::651706774390:role/microservices-project-dev-tf-role
source_profile = default
region = us-east-1

Step 3: Use that profile
export AWS_PROFILE=terraform-admin
aws eks update-kubeconfig --region us-east-1 --name effulgencetech-dev
kubectl get nodes


âœ… This will work immediately
âœ… No need to add IAM users to aws-auth
âœ… Same identity as Terraform & GitHub Actions

âœ… Option 2 (Quick fix): Add your IAM user to aws-auth

This is fine for dev only, but not ideal long-term.

map_users = [
  {
    userarn  = "arn:aws:iam::651706774390:user/Mustydain"
    username = "mustydain"
    groups   = ["system:masters"]
  }
]


Then:

terraform apply
aws eks update-kubeconfig --region us-east-1 --name effulgencetech-dev
kubectl get nodes

Why your aws-auth module appeared to fail

This is the classic chicken-and-egg issue:

aws-auth requires Kubernetes access

Kubernetes access requires aws-auth

Your module solved this correctly using:

enable_aws_auth_bootstrap = true
bootstrap_role_arn       = microservices-project-dev-tf-role


So Terraform did succeed â€” but your local user is still outside the trust boundary.

Final verdict (what you should do)

Since you are:

Using Terraform

Using GitHub Actions OIDC

Managing EKS professionally

ğŸ‘‰ DO NOT add IAM users
ğŸ‘‰ Always assume the Terraform / CI role

This keeps:

Local access

CI access

RBAC

Audit trail

all aligned.

Quick sanity check (run this)
aws sts get-caller-identity


If the ARN is not:

role/microservices-project-dev-tf-role


then Kubernetes will continue to reject you.


{
  "Effect": "Allow",
  "Action": "sts:AssumeRole",
  "Resource": "arn:aws:iam::651706774390:role/microservices-project-dev-tf-role"
}
we are going with your recommended. below is the permissions of the iam user that am using. the user has admin access. do we still need to amend its permissions. see its details below. if we still need to let the user to assume the tf role, then add the above policy you suggested.
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "*",
            "Resource": "*"
        }
    ]
}

Great question â€” and youâ€™re thinking about this exactly the right way ğŸ‘
Letâ€™s be very precise here, because this is a subtle but important AWS IAM rule.

Short answer (clear and correct)

YES â€” you still need to explicitly allow the user to assume the Terraform role.

Having AdministratorAccess on the user is NOT sufficient by itself.

Why AdminAccess is NOT enough

Your user policy:

{
  "Effect": "Allow",
  "Action": "*",
  "Resource": "*"
}


âœ… Allows the user to call STS:AssumeRole

âŒ Does NOT mean the role trusts the user

AWS role assumption requires TWO things:

Caller permission (user side)

Trust policy (role side) â† this is the missing piece 99% of the time

The missing requirement (critical)

Even if a user is admin:

A role will still reject AssumeRole unless the roleâ€™s trust policy explicitly allows it.

So the real question is:

ğŸ‘‰ Does microservices-project-dev-tf-role trust your IAM user?

What actually needs to be checked (and probably fixed)
1ï¸âƒ£ User side (already OK)

Your admin policy already allows:

"Action": "sts:AssumeRole"


So NO change required on the user policy âŒ
(Adding the policy you pasted is optional but redundant.)

2ï¸âƒ£ Role trust policy (this is the key)

Go to:

IAM â†’ Roles â†’ microservices-project-dev-tf-role â†’ Trust relationships


If you see ONLY something like this:

{
  "Effect": "Allow",
  "Principal": {
    "Federated": "arn:aws:iam::651706774390:oidc-provider/token.actions.githubusercontent.com"
  },
  "Action": "sts:AssumeRoleWithWebIdentity"
}


ğŸ‘‰ Then your IAM user CANNOT assume this role, no matter how much admin access it has.

Correct trust policy (GitHub Actions + you)

You want both:

GitHub Actions (OIDC)

Your IAM user (for local kubectl & terraform)

Recommended trust policy
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::651706774390:oidc-provider/token.actions.githubusercontent.com"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "token.actions.githubusercontent.com:aud": "sts.amazonaws.com"
        }
      }
    },
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::651706774390:user/Mustydain"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}


âœ… GitHub Actions still works
âœ… You can now assume the role locally
âœ… No need to touch aws-auth users

After fixing the trust policy (very important steps)
# assume the role
export AWS_PROFILE=terraform-admin

# regenerate kubeconfig
aws eks update-kubeconfig \
  --region us-east-1 \
  --name effulgencetech-dev

# test
kubectl get nodes


This will work immediately.

Why this is the BEST approach (architecturally)

âœ” One Kubernetes admin identity
âœ” No IAM users in aws-auth
âœ” Clean audit trail
âœ” Same permissions in:

Terraform

GitHub Actions

kubectl

ArgoCD bootstrap

This is exactly how production EKS environments are run.

Final verdict
Question	Answer
Does admin user need extra permissions?	âŒ No
Does the role need to trust the user?	âœ… YES
Should you add IAM users to aws-auth?	âŒ NO
Is this the right long-term design?	âœ… 100%

role trust policy updated. walk me through the next 2 key steps.
with step 2 below, am supposed to vi into ~/.aws/config and paste the role details?
Step 2: Configure AWS profile

~/.aws/config

[profile terraform-admin]
role_arn = arn:aws:iam::651706774390:role/microservices-project-dev-tf-role
source_profile = default
region = us-east-1

Step 3: Use that profile
export AWS_PROFILE=terraform-admin
aws eks update-kubeconfig --region us-east-1 --name effulgencetech-dev
kubectl get nodes


| Old (legacy)                        | New (recommended)               |
| ----------------------------------- | ------------------------------- |
| `aws-auth` ConfigMap                | **EKS Access Entries**          |
| `system:masters`                    | **AWS-managed access policies** |
| Kubernetes provider bootstrap hacks | **Native IAM â†’ EKS auth**       |
| Fragile                             | **Stable & supported**          |


Optional next upgrades (highly recommended)

If you want to go further, next we can:

Replace admin with least-privilege access

Namespace-scoped access for ArgoCD

Separate CI vs human access

Multi-account GitHub Actions access

Pod Identity (IRSA â†’ EKS Pod Identity)

Just tell me which one you want next.

# Used earlier, moved to access enteries
# Build YAML-friendly mapRoles and mapUsers strings. The aws-auth ConfigMap expects YAML in these fields.
locals {
  # Convert each map_roles object to a YAML structure when encoded
  role_entries = [
    for r in var.map_roles : {
      rolearn  = r.rolearn
      username = r.username
      groups   = r.groups
    }
  ]

  user_entries = [
    for u in var.map_users : {
      userarn  = u.userarn
      username = u.username
      groups   = u.groups
    }
  ]
  # Use yamlencode to produce YAML text (kubernetes ConfigMap expects YAML string)
  map_roles_yaml = length(local.role_entries) > 0 ? yamlencode(local.role_entries) : ""
  map_users_yaml = length(local.user_entries) > 0 ? yamlencode(local.user_entries) : ""
}

# Configure an in-module kubernetes provider that authenticates via aws eks get-token with role-arn
provider "kubernetes" {
  host = var.cluster_details.endpoint

  cluster_ca_certificate = base64decode(var.cluster_details.certificate_authority_data)

  exec {
    api_version = "client.authentication.k8s.io/v1beta1"
    command     = "aws"
    args        = [
      "eks",
      "get-token",
      "--cluster-name",
      var.cluster_name,
      "--role-arn",
      var.bootstrap_role_arn
    ]
  }
}

# Create or update aws-auth ConfigMap in kube-system
resource "kubernetes_config_map_v1" "aws_auth" {
  count = var.enable_aws_auth_bootstrap ? 1 : 0

  metadata {
    name      = "aws-auth"
    namespace = "kube-system"
  }

  data = {
    # Provide empty strings if none to avoid nulls
    mapRoles = local.map_roles_yaml
    mapUsers = local.map_users_yaml
  }

  lifecycle {
    create_before_destroy = false
    prevent_destroy      = true
  }

  # Ensure cluster exists before we attempt to apply
  depends_on = [var.cluster_name]
}
