Yes â€” the way youâ€™re getting the OIDC issuer is correct, but letâ€™s verify the format and common pitfalls.

1ï¸âƒ£ How Terraform outputs it:
output "oidc_provider_url" {
  value = aws_eks_cluster.dev_cluster.identity[0].oidc[0].issuer
}


This returns something like:

https://oidc.eks.us-east-1.amazonaws.com/id/7A2C6AB51D3637BC07FA403A62BA82F9


âœ… This is expected. It includes https://.

2ï¸âƒ£ What AWS IAM expects in the trust policy:

For IRSA, the Principal condition must not include https:// when referencing the issuer for :sub and :aud conditions.

So in Terraform, we must strip it when creating the role policy:

locals {
  oidc_provider_id = replace(aws_eks_cluster.dev_cluster.identity[0].oidc[0].issuer, "https://", "")
}


Then in the IAM policy:

variable = "${local.oidc_provider_id}:sub"
values   = ["system:serviceaccount:kube-system:aws-load-balancer-controller"]

3ï¸âƒ£ How to double-check from CLI:
aws eks describe-cluster --name effulgencetech-dev --query "cluster.identity.oidc.issuer" --output text


Should return something like:

https://oidc.eks.us-east-1.amazonaws.com/id/7A2C6AB51D3637BC07FA403A62BA82F9


âœ… If it matches your Terraform output, the cluster is correct.

âš ï¸ Only thing to fix is removing https:// when building the IAM trust policy.


âœ… The ONE missing piece (minimal & modern)

You need one Kubernetes ClusterRoleBinding that maps your role to cluster-admin.

ğŸ” Create this ONCE (recommended via Terraform or kubectl)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: terraform-admin
subjects:
  - kind: User
    name: arn:aws:iam::651706774390:role/microservices-project-dev-tf-role
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io


Apply with:

kubectl apply -f clusterrolebinding.yaml

ğŸ¯ What happens immediately after

AWS Console:

Pods âœ”

Nodes âœ”

Services âœ”

ConfigMaps âœ”

kubectl works

No aws-auth ConfigMap

No legacy mappings

Fully modern EKS access model

ğŸ”¥ Important best-practice note (youâ€™re doing it right)

Your setup now follows AWSâ€™s recommended 2024+ model for Amazon EKS:

âœ” IAM Access Entries
âœ” No aws-auth dependency
âœ” Explicit RBAC bindings
âœ” Clean separation of infra vs access

ğŸ§  Final mental model (remember this)

IAM Access Entry = who may authenticate
Kubernetes RBAC = what they may see/do

Both are required.